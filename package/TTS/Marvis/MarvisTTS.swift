import AVFoundation
import Foundation
import Hub
import MLX
import MLXLMCommon
import MLXNN
import Tokenizers

// MARK: - Main Class

public final class MarvisTTS: Module {
  public struct GenerationResult: Sendable {
    public let audio: [Float]
    public let sampleRate: Int
    public let sampleCount: Int
    public let frameCount: Int
    public let audioDuration: TimeInterval
    public let realTimeFactor: Double
    public let processingTime: Double
  }

  // MARK: - Public Properties

  public let sampleRate: Double

  // MARK: - Private Properties

  private let model: MarvisModel
  private let _promptURLs: [URL]?
  private let textTokenizer: any Tokenizer
  private let audioTokenizer: MimiTokenizer
  private let streamingDecoder: MimiStreamingDecoder
  private var boundVoice: MarvisEngine.Voice? = .conversationalA
  private var boundRefAudio: MLXArray?
  private var boundRefText: String?
  private var boundQuality: MarvisEngine.QualityLevel = .maximum

  // MARK: - Initializers

  init(
    config: MarvisModelArgs,
    repoId: String,
    promptURLs: [URL]? = nil,
    progressHandler: @escaping (Progress) -> Void,
  ) async throws {
    model = try MarvisModel(config: config)
    _promptURLs = promptURLs
    textTokenizer = try await loadTokenizer(configuration: ModelConfiguration(id: repoId), hub: HubApi.shared)
    audioTokenizer = try await MimiTokenizer(Mimi.fromPretrained(progressHandler: progressHandler))
    streamingDecoder = MimiStreamingDecoder(audioTokenizer.codec)
    sampleRate = audioTokenizer.codec.cfg.sampleRate

    super.init()

    try model.resetCaches()
  }

  convenience init(
    voice: MarvisEngine.Voice = .conversationalA,
    model: String = MarvisEngine.ModelVariant.default.repoId,
    progressHandler: @escaping (Progress) -> Void = { _ in },
  ) async throws {
    let (args, prompts, weightFileURL) = try await Self.snapshotAndConfig(repoId: model, progressHandler: progressHandler)
    try await self.init(config: args, repoId: model, promptURLs: prompts, progressHandler: progressHandler)
    try installWeights(args: args, weightFileURL: weightFileURL)

    boundVoice = voice
    boundRefAudio = nil
    boundRefText = nil
  }

  convenience init(
    refAudio: MLXArray,
    refText: String,
    model: String = MarvisEngine.ModelVariant.default.repoId,
    progressHandler: @escaping (Progress) -> Void = { _ in },
  ) async throws {
    let (args, prompts, weightFileURL) = try await Self.snapshotAndConfig(repoId: model, progressHandler: progressHandler)
    try await self.init(config: args, repoId: model, promptURLs: prompts, progressHandler: progressHandler)
    try installWeights(args: args, weightFileURL: weightFileURL)

    boundVoice = nil
    boundRefAudio = refAudio
    boundRefText = refText
  }
}

// MARK: - Public API

extension MarvisTTS {
  /// Manually triggers memory cleanup for this TTS instance
  func cleanUpMemory() throws {
    try model.resetCaches()
    streamingDecoder.reset()
  }

  /// Generate audio from text
  func generateAudio(
    text: String,
    voice: MarvisEngine.Voice,
    quality: MarvisEngine.QualityLevel? = nil,
    splitPattern: String? = #"(\n+)"#,
  ) throws -> GenerationResult {
    let pieces = splitText(text, pattern: splitPattern)
    let results = try generateCore(
      text: pieces,
      voice: voice,
      refAudio: nil,
      refText: nil,
      qualityLevel: quality ?? boundQuality,
      stream: false,
      streamingInterval: 0.5,
      onStreamingResult: nil,
    )
    return Self.mergeResults(results)
  }

  /// Generate audio with streaming - yields results via callback
  func generateAudioStream(
    text: String,
    voice: MarvisEngine.Voice,
    quality: MarvisEngine.QualityLevel? = nil,
    interval: Double = 0.5,
    splitPattern: String? = #"(\n+)"#,
    onResult: @escaping @Sendable (GenerationResult) -> Void,
  ) throws {
    let pieces = splitText(text, pattern: splitPattern)
    _ = try generateCore(
      text: pieces,
      voice: voice,
      refAudio: nil,
      refText: nil,
      qualityLevel: quality ?? boundQuality,
      stream: true,
      streamingInterval: interval,
      onStreamingResult: onResult,
    )
  }

  /// Creates a Marvis session and binds a default voice.
  static func make(
    voice: MarvisEngine.Voice = .conversationalA,
    repoId: String = "Marvis-AI/marvis-tts-250m-v0.1",
    progressHandler: @escaping (Progress) -> Void = { _ in },
  ) async throws -> MarvisTTS {
    let engine = try await fromPretrained(repoId: repoId, progressHandler: progressHandler)
    engine.boundVoice = voice
    engine.boundRefAudio = nil
    engine.boundRefText = nil
    return engine
  }

  /// Creates a Marvis session and binds a custom reference voice.
  static func make(
    refAudio: MLXArray,
    refText: String,
    repoId: String = "Marvis-AI/marvis-tts-250m-v0.1",
    progressHandler: @escaping (Progress) -> Void = { _ in },
  ) async throws -> MarvisTTS {
    let engine = try await fromPretrained(repoId: repoId, progressHandler: progressHandler)
    engine.boundVoice = nil
    engine.boundRefAudio = refAudio
    engine.boundRefText = refText
    return engine
  }

  static func fromPretrained(
    repoId: String = "Marvis-AI/marvis-tts-250m-v0.1",
    progressHandler: @escaping (Progress) -> Void,
  ) async throws -> MarvisTTS {
    let (args, prompts, weightFileURL) = try await snapshotAndConfig(repoId: repoId, progressHandler: progressHandler)
    let model = try await MarvisTTS(config: args, repoId: repoId, promptURLs: prompts, progressHandler: progressHandler)
    try model.installWeights(args: args, weightFileURL: weightFileURL)
    return model
  }
}

// MARK: - Private Helpers

private extension MarvisTTS {
  // MARK: - Model Loading

  static func snapshotAndConfig(
    repoId: String,
    progressHandler: @escaping (Progress) -> Void,
  ) async throws -> (args: MarvisModelArgs, promptURLs: [URL], weightFileURL: URL) {
    let modelDirectoryURL = try await Hub.snapshot(from: repoId, progressHandler: progressHandler)
    let weightFileURL = modelDirectoryURL.appending(path: "model.safetensors")
    let promptDir = modelDirectoryURL.appending(path: "prompts", directoryHint: .isDirectory)
    var audioPromptURLs: [URL] = []
    for url in try FileManager.default.contentsOfDirectory(at: promptDir, includingPropertiesForKeys: nil) where url.pathExtension == "wav" {
      audioPromptURLs.append(url)
    }
    let configFileURL = modelDirectoryURL.appending(path: "config.json")
    let args = try JSONDecoder().decode(MarvisModelArgs.self, from: Data(contentsOf: configFileURL))
    return (args, audioPromptURLs, weightFileURL)
  }

  func installWeights(args: MarvisModelArgs, weightFileURL: URL) throws {
    var weights: [String: MLXArray] = [:]
    let w = try loadArrays(url: weightFileURL)
    for (k, v) in w {
      weights[k] = v
    }

    func extractInt(from value: JSONValue?) -> Int? {
      guard let value else { return nil }
      switch value {
        case let .number(d):
          return Int(d)
        case let .string(s):
          return Int(s)
        default:
          return nil
      }
    }

    if let quantization = args.quantization,
       let groupSize = extractInt(from: quantization["group_size"]),
       let bits = extractInt(from: quantization["bits"])
    {
      quantize(model: self, groupSize: groupSize, bits: bits) { path, _ in
        weights["\(path).scales"] != nil
      }
    } else {
      weights = Self.sanitize(weights: weights)
    }

    let parameters = ModuleParameters.unflattened(weights)
    try update(parameters: parameters, verify: [.all])
    eval(self)
  }

  // MARK: - Factories (Apple-style ergonomics)

  /// Creates a Marvis session and binds a default voice.
  static func make(
    voice: MarvisEngine.Voice = .conversationalA,
    model: String = MarvisEngine.ModelVariant.default.repoId,
    progressHandler: @escaping (Progress) -> Void = { _ in },
  ) async throws -> MarvisTTS {
    let engine = try await fromPretrained(model: model, progressHandler: progressHandler)
    engine.boundVoice = voice
    engine.boundRefAudio = nil
    engine.boundRefText = nil
    return engine
  }

  /// Creates a Marvis session and binds a custom reference voice.
  static func make(
    refAudio: MLXArray,
    refText: String,
    model: String = MarvisEngine.ModelVariant.default.repoId,
    progressHandler: @escaping (Progress) -> Void = { _ in },
  ) async throws -> MarvisTTS {
    let engine = try await fromPretrained(model: model, progressHandler: progressHandler)
    engine.boundVoice = nil
    engine.boundRefAudio = refAudio
    engine.boundRefText = refText
    return engine
  }

  static func fromPretrained(model: String = MarvisEngine.ModelVariant.default.repoId, progressHandler: @escaping (Progress) -> Void) async throws -> MarvisTTS {
    let (args, prompts, weightFileURL) = try await snapshotAndConfig(repoId: model, progressHandler: progressHandler)
    let modelInstance = try await MarvisTTS(config: args, repoId: model, promptURLs: prompts, progressHandler: progressHandler)
    try modelInstance.installWeights(args: args, weightFileURL: weightFileURL)
    return modelInstance
  }

  static func sanitize(weights: [String: MLXArray]) -> [String: MLXArray] {
    var out: [String: MLXArray] = [:]
    out.reserveCapacity(weights.count)

    for (rawKey, v) in weights {
      var k = rawKey

      if !k.hasPrefix("model.") {
        k = "model." + k
      }

      if k.contains("attn") && !k.contains("self_attn") {
        k = k.replacingOccurrences(of: "attn", with: "self_attn")
        k = k.replacingOccurrences(of: "output_proj", with: "o_proj")
      }

      if k.contains("mlp") {
        k = k.replacingOccurrences(of: "w1", with: "gate_proj")
        k = k.replacingOccurrences(of: "w2", with: "down_proj")
        k = k.replacingOccurrences(of: "w3", with: "up_proj")
      }

      if k.contains("sa_norm") || k.contains("mlp_norm") {
        k = k.replacingOccurrences(of: "sa_norm", with: "input_layernorm")
        k = k.replacingOccurrences(of: "scale", with: "weight")
        k = k.replacingOccurrences(of: "mlp_norm", with: "post_attention_layernorm")
        k = k.replacingOccurrences(of: "scale", with: "weight")
      }

      if k.contains("decoder.norm") || k.contains("backbone.norm") {
        k = k.replacingOccurrences(of: "scale", with: "weight")
      }

      out[k] = v
    }

    return out
  }

  // MARK: - Tokenization

  func tokenizeTextSegment(text: String, speaker: Int) -> (MLXArray, MLXArray) {
    let K = model.args.audioNumCodebooks
    let frameW = K + 1

    let prompt = "[\(speaker)]" + text
    let ids = MLXArray(textTokenizer.encode(text: prompt))

    let T = ids.shape[0]
    var frame = MLXArray.zeros([T, frameW], type: Int32.self)
    var mask = MLXArray.zeros([T, frameW], type: Bool.self)

    let lastCol = frameW - 1
    do {
      let left = split(frame, indices: [lastCol], axis: 1)[0]
      let right = split(frame, indices: [lastCol], axis: 1)[1]
      let tail = split(right, indices: [1], axis: 1)
      let after = (tail.count > 1) ? tail[1] : MLXArray.zeros([T, 0], type: Int32.self)
      frame = concatenated([left, ids.reshaped([T, 1]), after], axis: 1)
    }

    do {
      let ones = MLXArray.ones([T, 1], type: Bool.self)
      let left = split(mask, indices: [lastCol], axis: 1)[0]
      let right = split(mask, indices: [lastCol], axis: 1)[1]
      let tail = split(right, indices: [1], axis: 1)
      let after = (tail.count > 1) ? tail[1] : MLXArray.zeros([T, 0], type: Bool.self)
      mask = concatenated([left, ones, after], axis: 1)
    }

    return (frame, mask)
  }

  func tokenizeAudio(_ audio: MLXArray, addEOS: Bool = true) -> (MLXArray, MLXArray) {
    let K = model.args.audioNumCodebooks
    let frameW = K + 1

    let x = audio.reshaped([1, 1, audio.shape[0]])
    var codes = audioTokenizer.codec.encode(x) // [1, K, Tq]
    codes = split(codes, indices: [1], axis: 0)[0].reshaped([K, codes.shape[2]])

    if addEOS {
      let eos = MLXArray.zeros([K, 1], type: Int32.self)
      codes = concatenated([codes, eos], axis: 1) // [K, Tq+1]
    }

    let T = codes.shape[1]
    var frame = MLXArray.zeros([T, frameW], type: Int32.self) // [T, K+1]
    var mask = MLXArray.zeros([T, frameW], type: Bool.self)

    let codesT = swappedAxes(codes, 0, 1) // [T, K]
    if K > 0 {
      let leftLen = K
      let right = split(frame, indices: [leftLen], axis: 1)[1] // [T, 1]
      frame = concatenated([codesT, right], axis: 1)
    }
    if K > 0 {
      let ones = MLXArray.ones([T, K], type: Bool.self)
      let right = MLXArray.zeros([T, 1], type: Bool.self)
      mask = concatenated([ones, right], axis: 1)
    }

    return (frame, mask)
  }

  func tokenizeSegment(_ segment: Segment, addEOS: Bool = true) -> (MLXArray, MLXArray) {
    let (txt, txtMask) = tokenizeTextSegment(text: segment.text, speaker: segment.speaker)
    let (aud, audMask) = tokenizeAudio(segment.audio, addEOS: addEOS)
    return (concatenated([txt, aud], axis: 0), concatenated([txtMask, audMask], axis: 0))
  }

  func tokenizeStart(for segment: Segment) -> (tokens: MLXArray, mask: MLXArray, pos: MLXArray) {
    let (st, sm) = tokenizeSegment(segment, addEOS: false)
    let promptTokens = concatenated([st], axis: 0).asType(Int32.self) // [T, K+1]
    let promptMask = concatenated([sm], axis: 0).asType(Bool.self) // [T, K+1]
    let currTokens = expandedDimensions(promptTokens, axis: 0) // [1, T, K+1]
    let currMask = expandedDimensions(promptMask, axis: 0) // [1, T, K+1]
    let currPos = expandedDimensions(MLXArray.arange(promptTokens.shape[0]), axis: 0) // [1, T]
    return (currTokens, currMask, currPos)
  }

  // MARK: - Generation Context

  func makeContext(voice: MarvisEngine.Voice?, refAudio: MLXArray?, refText: String?) throws -> Segment {
    if let refAudio, let refText {
      return Segment(speaker: 0, text: refText, audio: refAudio)
    } else if let voice {
      var refAudioURL: URL?
      for promptURL in _promptURLs ?? [] {
        if promptURL.lastPathComponent == "\(voice.rawValue).wav" {
          refAudioURL = promptURL
          break
        }
      }
      guard let refAudioURL else { throw MarvisTTSError.voiceNotFound }

      let (sampleRate, audio) = try loadAudioArray(from: refAudioURL)
      guard abs(sampleRate - 24000) < .leastNonzeroMagnitude else {
        throw MarvisTTSError.invalidRefAudio("Reference audio must be single-channel (mono) 24kHz, in WAV format.")
      }
      let refTextURL = refAudioURL.deletingPathExtension().appendingPathExtension("txt")
      let text = try String(data: Data(contentsOf: refTextURL), encoding: .utf8)
      guard let text else { throw MarvisTTSError.voiceNotFound }
      return Segment(speaker: 0, text: text, audio: audio)
    }
    throw MarvisTTSError.voiceNotFound
  }

  // MARK: - Core Generation

  func generateCore(
    text: [String],
    voice: MarvisEngine.Voice?,
    refAudio: MLXArray?,
    refText: String?,
    qualityLevel: MarvisEngine.QualityLevel,
    stream: Bool,
    streamingInterval: Double,
    onStreamingResult: (@Sendable (GenerationResult) -> Void)?,
  ) throws -> [GenerationResult] {
    guard voice != nil || refAudio != nil else {
      throw MarvisTTSError.invalidArgument("`voice` or `refAudio`/`refText` must be specified.")
    }

    let base = try makeContext(voice: voice, refAudio: refAudio, refText: refText)
    // Note: Python Sesame uses top_k=50, Swift uses topP=0.8. Both use temperature=0.9.
    // This difference may produce slightly different generation characteristics.
    let sampleFn = TopPSampler(temperature: 0.9, topP: 0.8).sample
    let intervalTokens = Int(streamingInterval * 12.5)
    var results: [GenerationResult] = []

    for prompt in text {
      let generationText = (base.text + " " + prompt).trimmingCharacters(in: .whitespaces)
      let seg = Segment(speaker: 0, text: generationText, audio: base.audio)

      try model.resetCaches()
      if stream { streamingDecoder.reset() }

      let (tok, msk, pos) = tokenizeStart(for: seg)
      let r = try decodePrompt(
        currTokens: tok,
        currMask: msk,
        currPos: pos,
        qualityLevel: qualityLevel,
        stream: stream,
        streamingIntervalTokens: intervalTokens,
        sampler: sampleFn,
        onStreamingResult: onStreamingResult,
      )
      results.append(contentsOf: r)
    }

    try model.resetCaches()
    if stream { streamingDecoder.reset() }
    return results
  }

  func decodePrompt(
    currTokens startTokens: MLXArray,
    currMask startMask: MLXArray,
    currPos startPos: MLXArray,
    qualityLevel: MarvisEngine.QualityLevel,
    stream: Bool,
    streamingIntervalTokens: Int,
    sampler sampleFn: (MLXArray) -> MLXArray,
    onStreamingResult: (@Sendable (GenerationResult) -> Void)?,
  ) throws -> [GenerationResult] {
    var results: [GenerationResult] = []

    var samplesFrames: [MLXArray] = [] // each is [B=1, K]
    var currTokens = startTokens
    var currMask = startMask
    var currPos = startPos

    var generatedCount = 0
    var yieldedCount = 0
    let maxAudioFrames = Int(60000 / 80.0) // 12.5 fps, 80 ms per frame
    let maxSeqLen = 2048 - maxAudioFrames
    precondition(currTokens.shape[1] < maxSeqLen, "Inputs too long, must be below max_seq_len - max_audio_frames: \(maxSeqLen)")

    var startTime = CFAbsoluteTimeGetCurrent()
    var frameCount = 0

    for _ in 0 ..< maxAudioFrames {
      let frame = try model.generateFrame(
        maxCodebooks: qualityLevel.codebookCount,
        tokens: currTokens,
        tokensMask: currMask,
        sampler: sampleFn,
      ) // [1, K]

      // EOS if every codebook is 0
      if frame.sum().item(Int32.self) == 0 { break }

      samplesFrames.append(frame)
      frameCount += 1

      let zerosText = MLXArray.zeros([1, 1], type: Int32.self)
      let nextFrame = concatenated([frame, zerosText], axis: 1) // [1, K+1]
      currTokens = expandedDimensions(nextFrame, axis: 1) // [1, 1, K+1]

      let onesK = ones([1, frame.shape[1]], type: Bool.self)
      let zero1 = zeros([1, 1], type: Bool.self)
      let nextMask = concatenated([onesK, zero1], axis: 1) // [1, K+1]
      currMask = expandedDimensions(nextMask, axis: 1) // [1, 1, K+1]

      currPos = split(currPos, indices: [currPos.shape[1] - 1], axis: 1)[1] + MLXArray(1)

      generatedCount += 1

      if stream, (generatedCount - yieldedCount) >= streamingIntervalTokens {
        yieldedCount = generatedCount
        let gr = generateResultChunk(samplesFrames, start: startTime, streaming: true)
        results.append(gr)
        onStreamingResult?(gr)
        samplesFrames.removeAll(keepingCapacity: true)
        startTime = CFAbsoluteTimeGetCurrent()
      }
    }

    if !samplesFrames.isEmpty {
      let gr = generateResultChunk(samplesFrames, start: startTime, streaming: stream)
      if stream { onStreamingResult?(gr) } else { results.append(gr) }
    }

    return results
  }

  func generateResultChunk(_ frames: [MLXArray], start: CFTimeInterval, streaming: Bool) -> GenerationResult {
    let frameCount = frames.count

    var stacked = stacked(frames, axis: 0) // [F, 1, K]
    stacked = swappedAxes(stacked, 0, 1) // [1, F, K]
    stacked = swappedAxes(stacked, 1, 2) // [1, K, F]

    let audio1x1x = streaming
      ? streamingDecoder.decodeFrames(stacked) // [1, 1, S]
      : audioTokenizer.codec.decode(stacked) // [1, 1, S]

    let sampleCount = audio1x1x.shape[2]
    let audio = audio1x1x.reshaped([sampleCount]) // [S]

    let elapsed = CFAbsoluteTimeGetCurrent() - start
    let sr = Int(sampleRate)
    let audioSeconds = Double(sampleCount) / Double(sr)
    let rtf = (audioSeconds > 0) ? elapsed / audioSeconds : 0.0

    return GenerationResult(
      audio: audio.asArray(Float32.self),
      sampleRate: sr,
      sampleCount: sampleCount,
      frameCount: frameCount,
      audioDuration: audioSeconds,
      realTimeFactor: (rtf * 100).rounded() / 100,
      processingTime: elapsed,
    )
  }

  // MARK: - Text Processing

  func splitText(_ text: String, pattern: String?) -> [String] {
    if let pat = pattern, let re = try? NSRegularExpression(pattern: pat) {
      let full = text.trimmingCharacters(in: .whitespacesAndNewlines)
      let range = NSRange(full.startIndex ..< full.endIndex, in: full)
      let splits = re.split(full, range: range)
      return splits.isEmpty ? [full] : splits
    }
    return [text]
  }

  // MARK: - Result Merging

  static func mergeResults(_ parts: [GenerationResult]) -> GenerationResult {
    guard let first = parts.first else {
      return GenerationResult(
        audio: [], sampleRate: 24000, sampleCount: 0,
        frameCount: 0, audioDuration: 0, realTimeFactor: 0, processingTime: 0,
      )
    }
    if parts.count == 1 { return first }

    var samples: [Float] = []
    samples.reserveCapacity(parts.reduce(0) { $0 + $1.sampleCount })
    var sampleCount = 0
    var frameCount = 0
    var audioDuration: Double = 0
    var processingTime: Double = 0

    for r in parts {
      samples += r.audio
      sampleCount += r.sampleCount
      frameCount += r.frameCount
      audioDuration += r.audioDuration
      processingTime += r.processingTime
    }

    let rtf = audioDuration > 0 ? processingTime / audioDuration : 0
    return GenerationResult(
      audio: samples,
      sampleRate: first.sampleRate,
      sampleCount: sampleCount,
      frameCount: frameCount,
      audioDuration: audioDuration,
      realTimeFactor: (rtf * 100).rounded() / 100,
      processingTime: processingTime,
    )
  }
}

// MARK: - Supporting Types

private struct Segment {
  let speaker: Int
  let text: String
  let audio: MLXArray

  init(speaker: Int, text: String, audio: MLXArray) {
    self.speaker = speaker
    self.text = text
    self.audio = audio
  }
}

enum MarvisTTSError: Error, LocalizedError {
  case invalidArgument(String)
  case voiceNotFound
  case invalidRefAudio(String)

  var errorDescription: String? {
    switch self {
      case let .invalidArgument(msg):
        msg
      case .voiceNotFound:
        "Requested voice not found or missing reference assets."
      case let .invalidRefAudio(msg):
        msg
    }
  }
}

// MARK: - Private Extensions

private extension NSRegularExpression {
  func split(_ s: String, range: NSRange) -> [String] {
    var last = 0
    var parts: [String] = []
    enumerateMatches(in: s, options: [], range: range) { m, _, _ in
      guard let m else { return }
      let r = NSRange(location: last, length: m.range.location - last)
      if let rr = Range(r, in: s) {
        let piece = String(s[rr]).trimmingCharacters(in: .whitespacesAndNewlines)
        if !piece.isEmpty { parts.append(piece) }
      }
      last = m.range.upperBound
    }
    let tailR = NSRange(location: last, length: range.upperBound - last)
    if let rr = Range(tailR, in: s) {
      let piece = String(s[rr]).trimmingCharacters(in: .whitespacesAndNewlines)
      if !piece.isEmpty { parts.append(piece) }
    }
    return parts
  }
}
