// Copyright 2025 Resemble AI (original model implementation)
// Copyright Anthony DePasquale (MLX port)
// Ported to MLX from https://github.com/resemble-ai/chatterbox
// License: licenses/chatterbox.txt

// VoiceEncoder for Chatterbox Turbo
// Uses separate LSTM layers (lstm1, lstm2, lstm3) instead of stacked LSTM

import Foundation
import MLX
import MLXNN

// MARK: - Voice Encoder Turbo

/// LSTM-based voice encoder for speaker embeddings (Turbo version)
/// Uses three separate LSTM layers to match Turbo weight structure
class VoiceEncoderTurbo: Module {
  let config: VoiceEncConfig

  // Three separate LSTM layers (matches Python MLX weight structure)
  @ModuleInfo(key: "lstm1") var lstm1: LSTM
  @ModuleInfo(key: "lstm2") var lstm2: LSTM
  @ModuleInfo(key: "lstm3") var lstm3: LSTM
  @ModuleInfo(key: "proj") var proj: Linear

  // Cosine similarity scaling (learnable parameters)
  @ParameterInfo(key: "similarity_weight") var similarityWeight: MLXArray
  @ParameterInfo(key: "similarity_bias") var similarityBias: MLXArray

  init(config: VoiceEncConfig = VoiceEncConfig()) {
    self.config = config

    _lstm1.wrappedValue = LSTM(inputSize: config.numMels, hiddenSize: config.veHiddenSize)
    _lstm2.wrappedValue = LSTM(inputSize: config.veHiddenSize, hiddenSize: config.veHiddenSize)
    _lstm3.wrappedValue = LSTM(inputSize: config.veHiddenSize, hiddenSize: config.veHiddenSize)
    _proj.wrappedValue = Linear(config.veHiddenSize, config.speakerEmbedSize)

    _similarityWeight.wrappedValue = MLXArray([10.0])
    _similarityBias.wrappedValue = MLXArray([-5.0])
  }

  /// Computes the embeddings of a batch of partial utterances.
  ///
  /// - Parameter mels: Batch of unscaled mel spectrograms (B, T, M) where T is vePartialFrames
  /// - Returns: Embeddings as (B, E) where E is speakerEmbedSize. Embeddings are L2-normed.
  func callAsFunction(_ mels: MLXArray) -> MLXArray {
    if config.normalizedMels {
      let minVal = mels.min()
      let maxVal = mels.max()
      if minVal.item(Float.self) < 0 || maxVal.item(Float.self) > 1 {
        fatalError("Mels outside [0, 1]. Min=\(minVal), Max=\(maxVal)")
      }
    }

    // Pass through stacked LSTM layers
    // MLX LSTM returns (allHidden, allCell) where allHidden is (B, T, H)
    var (x, _) = lstm1(mels)
    (x, _) = lstm2(x)
    (x, _) = lstm3(x)

    // Get final hidden state (last timestep)
    let finalHidden = x[0..., -1, 0...] // (B, H)

    // Project
    var rawEmbeds = proj(finalHidden)

    // Apply ReLU if configured
    if config.veFinalRelu {
      rawEmbeds = relu(rawEmbeds)
    }

    // L2 normalize
    let norm = MLX.sqrt(MLX.sum(rawEmbeds * rawEmbeds, axis: 1, keepDims: true))
    let embeds = rawEmbeds / norm

    return embeds
  }

  /// Computes embeddings of a batch of full utterances.
  ///
  /// - Parameters:
  ///   - mels: (B, T, M) unscaled mels
  ///   - melLens: List of mel lengths for each batch item
  ///   - overlap: Overlap between partial windows (default 0.5)
  ///   - rate: Rate for frame step calculation
  ///   - minCoverage: Minimum coverage for partial windows (default 0.8)
  ///   - batchSize: Batch size for processing partials
  /// - Returns: (B, E) embeddings
  func inference(
    mels: MLXArray,
    melLens: [Int],
    overlap: Float = 0.5,
    rate: Float? = nil,
    minCoverage: Float = 0.8,
    batchSize: Int? = nil
  ) -> MLXArray {
    // Compute where to split the utterances into partials
    let frameStep = getFrameStep(overlap: overlap, rate: rate, config: config)

    var nPartialsList: [Int] = []
    var targetLens: [Int] = []

    for l in melLens {
      let (nP, tL) = getNumWins(nFrames: l, step: frameStep, minCoverage: minCoverage, config: config)
      nPartialsList.append(nP)
      targetLens.append(tL)
    }

    // Possibly pad the mels to reach the target lengths
    var paddedMels = mels
    let lenDiff = (targetLens.max() ?? 0) - mels.shape[1]
    if lenDiff > 0 {
      let pad = MLXArray.zeros([mels.shape[0], lenDiff, config.numMels])
      paddedMels = MLX.concatenated([mels, pad], axis: 1)
    }

    // Group all partials together
    var partialList: [MLXArray] = []

    for (idx, nPartial) in nPartialsList.enumerated() {
      if nPartial > 0 {
        let mel = paddedMels[idx]
        // Extract partials
        for p in 0 ..< nPartial {
          let start = p * frameStep
          let end = start + config.vePartialFrames
          let partial = mel[start ..< end, 0...]
          partialList.append(partial)
        }
      }
    }

    guard !partialList.isEmpty else {
      return MLXArray.zeros([melLens.count, config.speakerEmbedSize])
    }

    let partials = MLX.stacked(partialList, axis: 0) // (total_partials, T, M)

    // Forward the partials (in batches if needed)
    var partialEmbeds: MLXArray
    if batchSize == nil || batchSize! >= partials.shape[0] {
      partialEmbeds = callAsFunction(partials)
    } else {
      var embedChunks: [MLXArray] = []
      let bs = batchSize!
      for i in stride(from: 0, to: partials.shape[0], by: bs) {
        let end = min(i + bs, partials.shape[0])
        let chunk = partials[i ..< end]
        embedChunks.append(callAsFunction(chunk))
      }
      partialEmbeds = MLX.concatenated(embedChunks, axis: 0)
    }

    // Reduce the partial embeds into full embeds
    var slices = [0]
    for n in nPartialsList {
      slices.append(slices.last! + n)
    }

    // Reduce partials to embeddings
    let rawEmbeds = (0 ..< (slices.count - 1)).map { i -> MLXArray in
      MLX.mean(partialEmbeds[slices[i] ..< slices[i + 1]], axis: 0)
    }
    let rawEmbedsStacked = MLX.stacked(rawEmbeds, axis: 0)

    // L2-normalize the final embeds
    let norm = MLX.sqrt(MLX.sum(rawEmbedsStacked * rawEmbedsStacked, axis: 1, keepDims: true))
    let embeds = rawEmbedsStacked / norm

    return embeds
  }

  /// Takes L2-normalized utterance embeddings, computes mean and L2-normalizes to get a speaker embedding.
  static func uttToSpkEmbed(_ uttEmbeds: MLXArray) -> MLXArray {
    precondition(uttEmbeds.ndim == 2, "Expected 2D tensor for utterance embeddings")
    let mean = MLX.mean(uttEmbeds, axis: 0)
    let norm = MLX.sqrt(MLX.sum(mean * mean))
    return mean / norm
  }

  /// Convenience function for deriving utterance or speaker embeddings from mel spectrograms.
  func embedsFromMels(
    mels: [MLXArray],
    melLens: [Int]? = nil,
    asSpk: Bool = false,
    batchSize: Int = 32,
    overlap: Float = 0.5,
    rate: Float? = nil,
    minCoverage: Float = 0.8
  ) -> MLXArray {
    let actualMelLens = melLens ?? mels.map { $0.shape[0] }
    let maxLen = actualMelLens.max() ?? 0

    // Pad and stack
    var padded: [MLXArray] = []
    for mel in mels {
      if mel.shape[0] < maxLen {
        let pad = MLXArray.zeros([maxLen - mel.shape[0], mel.shape[1]])
        padded.append(MLX.concatenated([mel, pad], axis: 0))
      } else {
        padded.append(mel)
      }
    }
    let melStacked = MLX.stacked(padded, axis: 0)

    // Embed them
    let uttEmbeds = inference(
      mels: melStacked,
      melLens: actualMelLens,
      overlap: overlap,
      rate: rate,
      minCoverage: minCoverage,
      batchSize: batchSize
    )

    return asSpk ? VoiceEncoderTurbo.uttToSpkEmbed(uttEmbeds) : uttEmbeds
  }

  /// Wrapper around embedsFromMels that first converts waveforms to mel spectrograms.
  func embedsFromWavs(
    wavs: [MLXArray],
    asSpk: Bool = false,
    batchSize: Int = 32,
    rate: Float = 1.3
  ) -> MLXArray {
    // Convert waveforms to mel spectrograms
    var mels: [MLXArray] = []
    for wav in wavs {
      let mel = voiceEncoderMelspectrogram(wav: wav, config: config)
      // Transpose from (M, T) to (T, M)
      mels.append(mel.transposed())
    }

    return embedsFromMels(mels: mels, asSpk: asSpk, batchSize: batchSize, rate: rate)
  }
}
