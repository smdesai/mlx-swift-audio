// Copyright ¬Æ Canopy Labs (original model implementation)
// Ported to MLX from https://github.com/canopyai/Orpheus-TTS
// Copyright ¬© 2024 Prince Canuma and contributors to Blaizzy/mlx-audio
// Copyright ¬© Anthony DePasquale
// License: licenses/orpheus.txt

import Foundation
@preconcurrency import MLX
@preconcurrency import MLXLMCommon
@preconcurrency import MLXNN
import MLXRandom
import Synchronization

// Orpheus TTS - Swift implementation of the Orpheus 3B model

// MARK: - Profiling Helper

enum Profiler {
  static let enabled: Bool = false

  static func time<T>(_ label: String, _ block: () throws -> T) rethrows -> T {
    guard enabled else { return try block() }

    let start = CFAbsoluteTimeGetCurrent()
    let result = try block()
    let end = CFAbsoluteTimeGetCurrent()
    let duration = (end - start) * 1000 // Convert to milliseconds
    Log.perf.debug("‚è±Ô∏è [PROFILE] \(label): \(duration.formatted(decimals: 2)) ms")
    return result
  }

  static func timeAsync<T>(_ label: String, _ block: () async throws -> T) async rethrows -> T {
    guard enabled else { return try await block() }

    let start = CFAbsoluteTimeGetCurrent()
    let result = try await block()
    let end = CFAbsoluteTimeGetCurrent()
    let duration = (end - start) * 1000 // Convert to milliseconds
    Log.perf.debug("‚è±Ô∏è [PROFILE] \(label): \(duration.formatted(decimals: 2)) ms")
    return result
  }
}

/// Wrapper for inference state that contains non-Sendable MLX types.
/// Marked @unchecked Sendable because all access is controlled within the actor.
private struct InferenceState: @unchecked Sendable {
  var cache: [KVCache]
  var logits: MLXArray
  var currentIds: MLXArray
}

actor OrpheusTTS {
  enum OrpheusTTSError: LocalizedError {
    case tooManyTokens
    case weightsNotAvailable
    case modelNotInitialized
    case weightLoadingFailed(String)

    var errorDescription: String? {
      switch self {
        case .tooManyTokens:
          "Input text exceeds maximum token limit"
        case .weightsNotAvailable:
          "Model weights not available"
        case .modelNotInitialized:
          "Model has not been initialized"
        case let .weightLoadingFailed(message):
          "Failed to load model weights: \(message)"
      }
    }
  }

  // MARK: - Constants

  private static let maxTokenCount = 1200
  private static let sampleRate = 24000
  private static let startToken = 128_259
  private static let endToken = 128_258
  private static let padToken = 128_263
  private static let audioStartToken = 128_261
  private static let audioEndToken = 128_262
  private static let voicePrefixToken = 128_260
  private static let repetitionContextSize = 20
  private static let codeOffset = 128_266
  private static let audioCodeDataStartMarker = 128_257

  // MARK: - Properties

  // Model components are nonisolated(unsafe) because they contain non-Sendable types (MLXArray)
  // but are only accessed within the actor's methods
  private nonisolated(unsafe) let model: OrpheusLMHeadModel
  private nonisolated(unsafe) let snacDecoder: SNACDecoder
  private var chosenVoice: OrpheusEngine.Voice?
  private let tokenizer: OrpheusTokenizer

  private init(model: OrpheusLMHeadModel, snacDecoder: SNACDecoder, tokenizer: OrpheusTokenizer) {
    self.model = model
    self.snacDecoder = snacDecoder
    self.tokenizer = tokenizer
  }

  static func load(
    repoId: String = OrpheusWeightLoader.defaultRepoId,
    snacRepoId: String = SNACDecoder.defaultRepoId,
    progressHandler: @escaping @Sendable (Progress) -> Void = { _ in },
  ) async throws -> OrpheusTTS {
    // Load model weights and quantization config from Hub (single snapshot call)
    let (loadedWeights, quantConfig) = try await Profiler.timeAsync("Weight and config loading") {
      try await OrpheusWeightLoader.load(repoId: repoId, progressHandler: progressHandler)
    }

    // Load SNAC decoder weights and config
    let snacConfig = try await Profiler.timeAsync("SNAC config loading") {
      try await SNACDecoder.loadConfig(repoId: snacRepoId, progressHandler: progressHandler)
    }
    let snacWeights = try await Profiler.timeAsync("SNAC weights loading") {
      try await SNACDecoder.loadWeights(repoId: snacRepoId, progressHandler: progressHandler)
    }

    // Initialize SNAC decoder using standard Module pattern
    let snacDecoder = Profiler.time("SNAC decoder init") {
      SNACDecoder(config: snacConfig)
    }

    // Sanitize and load weights using standard MLX pattern
    try Profiler.time("SNAC weight loading into model") {
      let (decoderWeights, quantizerWeights) = SNACDecoder.sanitizeWeights(snacWeights, noise: snacConfig.noise)
      let snacParameters = ModuleParameters.unflattened(decoderWeights)
      try snacDecoder.update(parameters: snacParameters, verify: .noUnusedKeys)
      snacDecoder.setQuantizerWeights(quantizerWeights)
    }

    // Evaluate SNAC model
    Profiler.time("SNAC model evaluation") {
      eval(snacDecoder)
    }

    // Download and load tokenizer files from Hub
    let tokenizerURLs = try await Profiler.timeAsync("Tokenizer download") {
      try await OrpheusTokenizer.download(repoId: repoId, progressHandler: progressHandler)
    }
    let tokenizer = try Profiler.time("Tokenizer init") {
      try OrpheusTokenizer(tokenizerURL: tokenizerURLs.tokenizerURL, configURL: tokenizerURLs.configURL)
    }

    // Initialize the model using Module pattern
    let model = Profiler.time("Model initialization") {
      OrpheusLMHeadModel()
    }

    // Apply quantization if config specifies it and weights have .scales
    if let quant = quantConfig {
      Log.model.info("Detected quantized model weights (\(quant.bits)-bit)")
      Profiler.time("Apply quantization") {
        quantize(model: model) { path, _ in
          loadedWeights["\(path).scales"] != nil ? (quant.groupSize, quant.bits, .affine) : nil
        }
      }
    }

    // Load weights into model using Module system
    try Profiler.time("Weight loading into model") {
      let parameters = ModuleParameters.unflattened(loadedWeights)
      try model.update(parameters: parameters, verify: [.all])
    }

    // Evaluate model to ensure weights are loaded
    Profiler.time("Model evaluation") {
      eval(model)
    }

    return OrpheusTTS(model: model, snacDecoder: snacDecoder, tokenizer: tokenizer)
  }

  func generate(text: String, voice: OrpheusEngine.Voice, temperature: Float = 0.6, topP: Float = 0.8) throws -> TTSGenerationResult {
    let startTime = CFAbsoluteTimeGetCurrent()

    // Split text into sentences
    let sentences = SentenceTokenizer.splitIntoSentences(text: text)

    var allAudio: [Float] = []
    for sentence in sentences {
      // Check for cancellation between sentences
      if Task.isCancelled {
        throw CancellationError()
      }

      let result = try generateChunk(text: sentence, voice: voice, temperature: temperature, topP: topP)
      allAudio.append(contentsOf: result.audio)
      MLXMemory.clearCache()
    }

    let processingTime = CFAbsoluteTimeGetCurrent() - startTime
    return TTSGenerationResult(
      audio: allAudio,
      sampleRate: Self.sampleRate,
      processingTime: processingTime,
    )
  }

  /// Generate audio as a stream of chunks (one per sentence)
  func generateStreaming(
    text: String,
    voice: OrpheusEngine.Voice,
    temperature: Float = 0.6,
    topP: Float = 0.8,
  ) -> AsyncThrowingStream<[Float], Error> {
    let sentences = SentenceTokenizer.splitIntoSentences(text: text)
    let sentenceIndex = Atomic<Int>(0)

    return AsyncThrowingStream {
      let i = sentenceIndex.wrappingAdd(1, ordering: .relaxed).oldValue
      guard i < sentences.count else { return nil }

      try Task.checkCancellation()

      let result = try await self.generateChunk(text: sentences[i], voice: voice, temperature: temperature, topP: topP)
      MLXMemory.clearCache()
      return result.audio
    }
  }

  /// Generate audio for a single text chunk (no splitting)
  private func generateChunk(text: String, voice: OrpheusEngine.Voice, temperature: Float, topP: Float) throws -> TTSGenerationResult {
    let startTime = CFAbsoluteTimeGetCurrent()

    // Prepare input with voice prefix
    let prompt = "\(voice.rawValue): \(text)"
    Log.tts.debug("Orpheus prompt: \(prompt)")

    let inputIdsTuple = Profiler.time("Tokenizer preparation") {
      tokenizer.prepareInputIds(prompts: [prompt])
    }

    // Convert the tokenizer output to a Swift [Int32]
    let currentIds = Profiler.time("Input IDs conversion") {
      let array = MLXArray(inputIdsTuple.0[0].asArray(Int32.self))
      return array.ndim == 1 ? array.reshaped([1, -1]) : array
    }

    Log.tts.debug("Input IDs: \(currentIds.shape) = \(currentIds.asArray(Int32.self))")

    // Initialize inference state with KV caches and initial forward pass
    // Using InferenceState wrapper to satisfy Swift concurrency checker
    var state = Profiler.time("Initial forward pass") {
      let cache = model.newCache()
      var logits = model(currentIds, cache: cache)
      // Get logits for the last token only
      logits = logits[0, -1].expandDims(at: 0)
      return InferenceState(cache: cache, logits: logits, currentIds: currentIds)
    }

    // Generate audio tokens
    var generatedTokensForPenalty: [Int32] = [] // For repetition penalty
    var i = 0

    let maxOutputTokens = Self.maxTokenCount // Define how many tokens to generate at most

    while i < maxOutputTokens {
      // Check for cancellation periodically
      if i % 50 == 0, Task.isCancelled {
        throw CancellationError()
      }

      let iterationStart = Profiler.enabled ? CFAbsoluteTimeGetCurrent() : 0

      let historyForRepetition = Profiler.time("History preparation") {
        MLXArray(generatedTokensForPenalty)
      }

      let samplingStart = Profiler.enabled ? CFAbsoluteTimeGetCurrent() : 0
      let nextTokenArray = sampleNextToken(
        logits: state.logits,
        history: historyForRepetition,
        temperature: temperature,
        topP: topP,
        repetitionPenalty: 1.3,
      )
      let samplingEnd = Profiler.enabled ? CFAbsoluteTimeGetCurrent() : 0
      let samplingDuration = Profiler.enabled ? (samplingEnd - samplingStart) * 1000 : 0
      if Profiler.enabled {
        Log.perf.debug("‚è±Ô∏è [PROFILE] Token sampling (iter \(i)): \(samplingDuration.formatted(decimals: 2)) ms")
      }

      // Double-buffering: Start forward pass BEFORE extracting token
      // GPU computes next step while we do CPU operations
      let forwardPassStart = Profiler.enabled ? CFAbsoluteTimeGetCurrent() : 0
      let nextInput = nextTokenArray.reshaped([1, 1])
      state.logits = model(nextInput, cache: state.cache)
      state.logits = state.logits.squeezed(axis: 1)
      asyncEval(state.logits, state.cache)
      let forwardPassEnd = Profiler.enabled ? CFAbsoluteTimeGetCurrent() : 0
      let forwardPassDuration = Profiler.enabled ? (forwardPassEnd - forwardPassStart) * 1000 : 0
      if Profiler.enabled {
        Log.perf.debug("‚è±Ô∏è [PROFILE] Forward pass (iter \(i)): \(forwardPassDuration.formatted(decimals: 2)) ms")
      }

      // NOW extract the token - GPU is already computing next step
      let nextToken: Int32 = Profiler.time("Token extraction") {
        let result: Int32 = nextTokenArray[0].item()
        return result
      }

      // Stop generation only at the general end-of-text token
      if nextToken == Self.endToken {
        let endArr = MLXArray([Self.endToken]).reshaped([1, 1])
        state.currentIds = MLX.concatenated([state.currentIds, endArr], axis: 1)
        if Profiler.enabled {
          Log.tts.debug("End token \(Self.endToken) encountered. Appending and breaking.")
        }
        break
      }

      // Add next token to the sequence for parsing and for model input
      Profiler.time("Token concatenation (iter \(i))") {
        let nextTokenForConcat = nextTokenArray.reshaped([1, 1])
        state.currentIds = MLX.concatenated([state.currentIds, nextTokenForConcat], axis: 1)
      }

      // Add to history for repetition penalty *after* it's been sampled
      Profiler.time("History update") {
        generatedTokensForPenalty.append(nextToken)
        if generatedTokensForPenalty.count > Self.repetitionContextSize {
          generatedTokensForPenalty.removeFirst()
        }
      }

      // Clear GPU cache periodically
      if (i + 1) % 50 == 0 {
        Profiler.time("GPU cache clear") {
          MLXMemory.clearCache()
        }
      }

      if Profiler.enabled {
        let iterationEnd = CFAbsoluteTimeGetCurrent()
        let iterationDuration = (iterationEnd - iterationStart) * 1000

        // Print detailed timing every 10 iterations or for first 5
        if i < 5 || i % 10 == 0 {
          Log.perf.debug("  üîÄ Iteration \(i): \(iterationDuration.formatted(decimals: 2)) ms total")
          Log.perf.debug("    üìä Forward: \(forwardPassDuration.formatted(decimals: 2)) ms")
          Log.perf.debug("    üéØ Token: \(nextToken)")
        }
      }

      i += 1
    }

    if i >= maxOutputTokens {
      Log.tts.warning("Reached max token count (\(maxOutputTokens)) during generation.")
    }

    // Parse the output into code lists
    let codeLists = Profiler.time("Output parsing") {
      parseOutput(tokens: state.currentIds.asArray(Int32.self).map { Int($0) })
    }

    // Generate audio using SNAC decoder
    let waveform = Profiler.time("SNAC decoding") {
      snacDecoder.decode(codes: codeLists)
    }

    waveform.eval()
    let processingTime = CFAbsoluteTimeGetCurrent() - startTime
    Log.perf.info("üèÅ [PROFILE] Total audio generation: \((processingTime * 1000).formatted(decimals: 2)) ms")

    return TTSGenerationResult(
      audio: waveform.asArray(Float.self),
      sampleRate: Self.sampleRate,
      processingTime: processingTime,
    )
  }

  private func sampleNextToken(
    logits: MLXArray,
    history: MLXArray,
    temperature: Float,
    topP: Float,
    repetitionPenalty: Float = 1.3,
  ) -> MLXArray {
    let samplingStart = Profiler.enabled ? CFAbsoluteTimeGetCurrent() : 0

    // Start with raw logits
    var currentLogits = logits

    // 1. Apply repetition penalty if needed
    if repetitionPenalty != 1.0, history.size > 0 {
      currentLogits = Profiler.time("Repetition penalty") {
        // Vectorised implementation to keep data on GPU/Metal.
        let indices = history // Int32 tensor with shape [K]
        let logits1D = currentLogits[0] // Shape [V]

        // Gather the logits corresponding to the history tokens.
        let gathered = MLX.take(logits1D, indices)

        // Compute updated logits according to the repetition penalty.
        let negMask = gathered .< 0
        let updated = MLX.where(
          negMask,
          gathered * repetitionPenalty,
          gathered / repetitionPenalty,
        )

        // Scatter the updated values back into the logits tensor using native subscript.
        logits1D[indices] = updated

        // Restore the [1, V] shape expected downstream.
        return logits1D.expandDims(at: 0)
      }
    }

    // 2. Apply temperature scaling
    let scaledLogits = Profiler.time("Temperature scaling") {
      currentLogits / max(temperature, 1e-6)
    }

    // 3. Apply top-p filtering
    var filteredLogits = scaledLogits
    if topP > 0.0, topP < 1.0 {
      filteredLogits = Profiler.time("Top-p filtering") {
        let vocabSize = scaledLogits.shape[1]
        if vocabSize > 1 {
          // Vectorised top-p filtering (no host round-trips).

          // 1. Probabilities.
          let probs = MLX.softmax(scaledLogits[0], axis: -1) // [V]

          // 2. Sort (descending).
          let sortedIdx = MLX.argSort(MLX.negative(probs)) // [V] Int32
          let sortedProbs = MLX.take(probs, sortedIdx) // [V]

          // 3. Cumulative sum.
          let cumProbs = sortedProbs.cumsum(axis: -1) // [V]

          // 4. Mask tokens occurring strictly after the cut-off.
          let gtMask = cumProbs .> topP // Bool [V]
          let gtMaskInt = gtMask.asType(.int32) // Int32 [V]
          let prefix = gtMaskInt.cumsum(axis: -1) // Int32 [V]
          let removeMaskSorted = prefix .> 1 // Bool [V]

          // 5. Bring mask back to original vocab order.
          let invIdx = MLX.argSort(sortedIdx) // [V]
          let removeMask = MLX.take(removeMaskSorted, invIdx) // Bool [V]

          // 6. Apply mask: set filtered logits to -inf.
          let negInfScalar = MLXArray(-Float.infinity) // scalar
          let logits1D = scaledLogits[0]
          let filtered1D = MLX.where(removeMask, negInfScalar, logits1D)

          // 7. Restore [1, V] shape expected downstream.
          return filtered1D.expandDims(at: 0)
        }
        return scaledLogits
      }
    }

    // 4. Sample from filtered distribution
    let nextTokenIdArray = Profiler.time("Categorical sampling") {
      MLXRandom.categorical(filteredLogits, count: 1)
    }

    if Profiler.enabled {
      let samplingEnd = CFAbsoluteTimeGetCurrent()
      let samplingDuration = (samplingEnd - samplingStart) * 1000
      Log.perf.debug("  üé≤ Sampling total: \(samplingDuration.formatted(decimals: 2)) ms")
    }

    return nextTokenIdArray
  }

  private func parseOutput(tokens: [Int]) -> [[Int]] {
    // Find the last occurrence of the audio start token as defined in Constants
    let lastStartIndex = tokens.lastIndex(of: Self.audioCodeDataStartMarker) ?? -1

    // Get tokens after the last start token
    let relevantTokens = lastStartIndex >= 0 ? Array(tokens[(lastStartIndex + 1)...]) : tokens

    // Filter out the general end token (128258) and ensure codes are valid (>= codeOffset)
    // Python's llama.py uses token_to_remove = 128258 and does not filter a separate audioEndToken.
    let filteredTokens = relevantTokens.filter { $0 != Self.endToken && $0 >= Self.codeOffset }

    // Ensure length is multiple of 7 by trimming
    let newLength = (filteredTokens.count / 7) * 7
    let trimmedTokens = Array(filteredTokens[..<newLength])

    // Subtract offset from all tokens
    let adjustedTokens = trimmedTokens.map { $0 - Self.codeOffset }

    // Split into layers based on the stride pattern
    var layer1: [Int] = []
    var layer2: [Int] = []
    var layer3: [Int] = []

    // Process codes in groups of 7
    for i in 0 ..< (adjustedTokens.count / 7) {
      let base = 7 * i
      layer1.append(adjustedTokens[base])
      layer2.append(adjustedTokens[base + 1] - 4096)
      layer3.append(adjustedTokens[base + 2] - 2 * 4096)
      layer3.append(adjustedTokens[base + 3] - 3 * 4096)
      layer2.append(adjustedTokens[base + 4] - 4 * 4096)
      layer3.append(adjustedTokens[base + 5] - 5 * 4096)
      layer3.append(adjustedTokens[base + 6] - 6 * 4096)
    }

    return [layer1, layer2, layer3]
  }
}
