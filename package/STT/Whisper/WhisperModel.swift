// Copyright © 2022 OpenAI (original model implementation)
// Copyright © Anthony DePasquale (MLX port)
// Ported to MLX from https://github.com/openai/whisper
// License: licenses/whisper.txt

import Foundation
import Hub
import MLX
import MLXLMCommon
import MLXNN

/// Main Whisper model containing AudioEncoder and TextDecoder
class WhisperModel: Module {
  @ModuleInfo var encoder: AudioEncoder
  @ModuleInfo var decoder: TextDecoder
  let dims: ModelDimensions

  // Path to the model directory (for loading tokenizer vocab)
  var modelDirectory: URL?

  // Alignment heads for word-level timestamps
  // Shape: (num_alignment_heads, 2) where each row is [layer_idx, head_idx]
  @ParameterInfo(key: "alignment_heads") var alignmentHeads: MLXArray

  init(dims: ModelDimensions) {
    self.dims = dims
    _encoder.wrappedValue = AudioEncoder(
      nMels: dims.n_mels,
      nCtx: dims.n_audio_ctx,
      nState: dims.n_audio_state,
      nHead: dims.n_audio_head,
      nLayer: dims.n_audio_layer
    )
    _decoder.wrappedValue = TextDecoder(
      nVocab: dims.n_vocab,
      nCtx: dims.n_text_ctx,
      nState: dims.n_text_state,
      nHead: dims.n_text_head,
      nLayer: dims.n_text_layer
    )

    // Initialize default alignment_heads (last half of decoder layers)
    // This matches Python's default: all_heads[n_text_layer // 2 :] = True
    // alignment_heads shape: (num_alignment_heads, 2) where each row is [layer_idx, head_idx]
    var defaultHeads: [[Int32]] = []
    let startLayer = dims.n_text_layer / 2
    for layer in startLayer ..< dims.n_text_layer {
      for head in 0 ..< dims.n_text_head {
        defaultHeads.append([Int32(layer), Int32(head)])
      }
    }
    _alignmentHeads.wrappedValue = MLXArray(defaultHeads.flatMap { $0 }).reshaped(defaultHeads.count, 2)
  }

  /// Encode audio features (matches Python's embed_audio method)
  ///
  /// - Parameter mel: Mel spectrogram (batch, n_mels, n_frames)
  /// - Returns: Encoded audio features (batch, n_audio_ctx, n_audio_state)
  func encode(_ mel: MLXArray) -> MLXArray {
    encoder(mel)
  }

  /// Decode tokens with audio features
  ///
  /// - Parameters:
  ///   - tokens: Token indices (batch, n_tokens)
  ///   - audioFeatures: Encoded audio features (batch, n_audio_ctx, n_audio_state)
  ///   - kvCache: Optional cached key/value tensors
  /// - Returns: Tuple of (logits, new_kv_cache, cross_attention_weights)
  func decode(
    _ tokens: MLXArray,
    audioFeatures: MLXArray,
    kvCache: [((MLXArray, MLXArray)?, (MLXArray, MLXArray)?)]? = nil
  ) -> (MLXArray, [((MLXArray, MLXArray)?, (MLXArray, MLXArray)?)], [MLXArray?]) {
    decoder(tokens, xa: audioFeatures, kvCache: kvCache)
  }

  /// Get logits for tokens given audio features (matches Python's logits method)
  ///
  /// - Parameters:
  ///   - tokens: Token indices (batch, n_tokens)
  ///   - audioFeatures: Encoded audio features (batch, n_audio_ctx, n_audio_state)
  /// - Returns: Logits (batch, n_tokens, n_vocab)
  func logits(_ tokens: MLXArray, audioFeatures: MLXArray) -> MLXArray {
    let (logits, _, _) = decode(tokens, audioFeatures: audioFeatures)
    return logits
  }

  /// Forward pass with cross-attention weights (for word-level timestamps)
  ///
  /// - Parameters:
  ///   - mel: Mel spectrogram (batch, n_mels, n_frames)
  ///   - tokens: Token indices (batch, n_tokens)
  /// - Returns: Tuple of (logits, cross_attention_weights)
  func forwardWithCrossQK(_ mel: MLXArray, tokens: MLXArray) -> (MLXArray, [MLXArray?]) {
    let audioFeatures = encode(mel)
    let (logits, _, crossQK) = decode(tokens, audioFeatures: audioFeatures)
    return (logits, crossQK)
  }

  /// Main forward pass (matches Python's __call__ method)
  ///
  /// - Parameters:
  ///   - mel: Mel spectrogram (batch, n_mels, n_frames)
  ///   - tokens: Token indices (batch, n_tokens)
  /// - Returns: Logits (batch, n_tokens, n_vocab)
  func callAsFunction(_ mel: MLXArray, tokens: MLXArray) -> MLXArray {
    logits(tokens, audioFeatures: encode(mel))
  }

  /// Whether this is a multilingual model
  ///
  /// Detection logic:
  /// - Multilingual models have n_vocab = 51866 (use multilingual.tiktoken)
  ///   - 50,257 base tokens + 1,609 special tokens = 51,866
  ///   - Models: tiny, base, small, medium, large, large-v2, large-v3, large-v3-turbo
  ///
  /// - English-only models have n_vocab = 51864 (use gpt2.tiktoken)
  ///   - 50,256 base tokens + 1,608 special tokens = 51,864
  ///   - Models: tiny.en, base.en, small.en, medium.en
  var isMultilingual: Bool {
    dims.n_vocab >= 51865
  }

  /// Number of supported languages
  var numLanguages: Int {
    dims.n_vocab - 51765 - (isMultilingual ? 1 : 0)
  }

  /// Set alignment heads for word-level timestamps
  ///
  /// - Parameter heads: Array of shape (num_heads, 2) where each row is [layer_idx, head_idx]
  func setAlignmentHeads(_ heads: MLXArray) {
    alignmentHeads = heads
  }

  /// Load Whisper model from Hugging Face Hub
  ///
  /// - Parameters:
  ///   - modelSize: Model size (tiny, base, small, medium, large, largeTurbo)
  ///   - quantization: Quantization level (fp16, 8bit, 4bit). Default is 4bit.
  ///   - progressHandler: Optional callback for download progress
  /// - Returns: Initialized WhisperModel with loaded weights
  static func load(
    modelSize: WhisperModelSize,
    quantization: WhisperQuantization = .q4,
    progressHandler: @escaping @Sendable (Progress) -> Void = { _ in }
  ) async throws -> WhisperModel {
    // Validate model is available (has safetensors weights)
    guard modelSize.isAvailable else {
      throw STTError.modelUnavailable(
        "Model '\(modelSize.rawValue)' requires .npz format which MLX Swift doesn't support. Available models: tiny, base, large-v3-turbo"
      )
    }

    let repoId = modelSize.repoId(quantization: quantization)
    Log.model.info("Loading Whisper from \(repoId)...")

    // Download model files from Hugging Face
    // All mlx-community Whisper models use:
    // - model.safetensors: Model weights
    // - config.json: Model configuration
    // - multilingual.tiktoken or gpt2.tiktoken: Tokenizer vocabulary
    let modelDirectory = try await HubConfiguration.shared.snapshot(
      from: repoId,
      matching: [
        "model.safetensors",
        "config.json",
        "multilingual.tiktoken",
        "gpt2.tiktoken",
      ],
      progressHandler: progressHandler
    )

    // Load config to get model dimensions
    let configURL = modelDirectory.appending(path: "config.json")
    let dims = try ModelDimensions.load(from: configURL)

    // Load weights from model.safetensors
    let modelSafetensors = modelDirectory.appending(path: "model.safetensors")
    guard FileManager.default.fileExists(atPath: modelSafetensors.path) else {
      throw STTError.modelUnavailable("model.safetensors not found in \(repoId)")
    }
    let weights = try MLX.loadArrays(url: modelSafetensors)

    // Initialize model
    let model = WhisperModel(dims: dims)

    // Apply quantization if weights are quantized and quantization level specifies bits
    let isQuantized = weights.keys.contains { $0.contains(".scales") }
    if isQuantized, let bits = quantization.bits {
      Log.model.info("Detected quantized Whisper model weights (\(bits)-bit)")
      quantize(model: model) { path, _ in
        weights["\(path).scales"] != nil ? (64, bits, .affine) : nil
      }
    }

    // Load weights into model
    let parameters = ModuleParameters.unflattened(weights)
    try model.update(parameters: parameters, verify: [.noUnusedKeys])

    // Set to eval mode for inference
    model.train(false)

    // Evaluate model to ensure weights are loaded
    eval(model)

    // Store model directory for tokenizer to find vocab files
    model.modelDirectory = modelDirectory

    Log.model.info("Whisper \(modelSize.rawValue) model loaded successfully")

    return model
  }

  /// Detect the spoken language in the audio
  ///
  /// Note: Language detection is only meaningful for multilingual models.
  /// For English-only models, this always returns ("en", 1.0).
  ///
  /// - Parameter mel: Mel spectrogram (batch, n_mels, n_frames)
  /// - Returns: Tuple of (language_code, probability)
  func detectLanguage(_ mel: MLXArray) -> (String, Float) {
    // English-only models don't have language tokens
    guard isMultilingual else {
      return ("en", 1.0)
    }

    // Compute token IDs dynamically (matching tokenizer logic)
    // Multilingual: base=50257, eot=50257, sot=50258
    // Language tokens start at sot+1 = 50259
    let baseVocabSize = 50257
    let sot = baseVocabSize + 1 // 50258
    let languageTokenStart = sot + 1 // 50259

    // Encode audio
    let audioFeatures = encode(mel)

    // Create SOT token for language detection
    let sotToken = MLXArray([Int32(sot)]).expandedDimensions(axis: 0)

    // Get logits for the first token after SOT
    let (logits, _, _) = decode(sotToken, audioFeatures: audioFeatures)

    // Get language token logits (numLanguages tokens, not hardcoded 100)
    let languageTokenEnd = languageTokenStart + numLanguages
    let languageLogits = logits[0, 0, languageTokenStart ..< languageTokenEnd]

    // Find the language with highest probability
    let probs = MLX.softmax(languageLogits, axis: -1)
    let maxIdx = MLX.argMax(probs).item(Int32.self)
    let maxProb = probs[Int(maxIdx)].item(Float.self)

    // Map index to language code using WHISPER_LANGUAGES
    let languageIdx = Int(maxIdx)
    let languageCode = languageIdx < numLanguages && languageIdx < WHISPER_LANGUAGES.count
      ? WHISPER_LANGUAGES[languageIdx].code : "en"

    return (languageCode, maxProb)
  }
}
